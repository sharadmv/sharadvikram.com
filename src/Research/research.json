{
  "publications": [
    {
      "title": "What are Bayesian neural network posteriors really like?",
      "authors": [
        "Pavel Izmailov",
        "Sharad Vikram",
        "Matthew Hoffman",
        "Andrew Gordon Wilson"
      ],
      "venue": "ICML 2021",
      "imgSrc": "/img/posterior.png",
      "description": "The posterior over Bayesian neural network (BNN) parameters is extremely high-dimensional and non-convex. For computational reasons, researchers approximate this posterior using inexpensive mini-batch methods such as mean-field variational inference or stochastic-gradient Markov chain Monte Carlo (SGMCMC). To investigate foundational questions in Bayesian deep learning, we instead use full batch Hamiltonian Monte Carlo (HMC) on modern architectures. We show that (1) BNNs can achieve significant performance gains over standard training and deep ensembles;(2) a single long HMC chain can provide a comparable representation of the posterior to multiple shorter chains;(3) in contrast to recent studies, we find posterior tempering is not needed for near-optimal performance, with little evidence for a “cold posterior” effect, which we show is largely an artifact of data augmentation;(4) BMA performance is robust to the choice of prior scale, and relatively similar for diagonal Gaussian, mixture of Gaussian, and logistic priors;(5) Bayesian neural networks show surprisingly poor generalization under domain shift;(6) while cheaper alternatives such as deep ensembles and SGMCMC can provide good generalization, their predictive distributions are distinct from HMC. Notably, deep ensemble predictive distributions are similarly close to HMC as standard SGLD, and closer than standard variational inference.",
      "arxivLink": "https://arxiv.org/abs/2104.14421"
    },
    {
      "title": "Automatic structured variational inference",
      "authors": [
        "Luca Ambrogioni", "Kate Lin", "Emily Fertig", "Sharad Vikram", "Max Hinne", "Dave Moore", "Marcel van Gerven"
      ],
      "venue": "AISTATS 2021",
      "imgSrc": "/img/asvi.png",
      "description": "Stochastic variational inference offers an attractive option as a default method for differentiable probabilistic programming. However, the performance of the variational approach depends on the choice of an appropriate variational family. Here, we introduce automatic structured variational inference (ASVI), a fully automated method for constructing structured variational families, inspired by the closed-form update in conjugate Bayesian models. These convex-update families incorporate the forward pass of the input probabilistic program and can therefore capture complex statistical dependencies. Convex-update families have the same space and time complexity as the input probabilistic program and are therefore tractable for a very large family of models including both continuous and discrete variables. We validate our automatic variational method on a wide range of low- and high-dimensional inference problems. We find that ASVI provides a clear improvement in performance when compared with other popular approaches such as the mean-field approach and inverse autoregressive flows. We provide an open source implementation of ASVI in TensorFlow Probability.",
      "arxivLink": "https://arxiv.org/abs/2002.00643"
    },
    {
      "title": "Automatic Differentiation Variational Inference with Mixtures",
      "authors": [
        "Warren Morningstar", "Sharad Vikram", "Cusuh Ham", "Andrew Gallagher", "Joshua Dillon"
      ],
      "venue": "AISTATS 2021",
      "imgSrc": "/img/madvi.png",
      "description": "Automatic Differentiation Variational Inference (ADVI) is a useful tool for efficiently learning probabilistic models in machine learning. Generally approximate posteriors learned by ADVI are forced to be unimodal in order to facilitate use of the reparameterization trick. In this paper, we show how stratified sampling may be used to enable mixture distributions as the approximate posterior, and derive a new lower bound on the evidence analogous to the importance weighted autoencoder (IWAE). We show that this \"SIWAE\" is a tighter bound than both IWAE and the traditional ELBO, both of which are special instances of this bound. We verify empirically that the traditional ELBO objective disfavors the presence of multimodal posterior distributions and may therefore not be able to fully capture structure in the latent space. Our experiments show that using the SIWAE objective allows the encoder to learn more complex distributions which regularly contain multimodality, resulting in higher accuracy and better calibration in the presence of incomplete, limited, or corrupted data.",
      "arxivLink": "https://proceedings.mlr.press/v130/morningstar21b.html"
    },
    {
      "title": "SOLAR: Deep Structured Latent Representations for Model-Based Reinforcement Learning",
      "authors": [
        "Marvin Zhang*",
        "Sharad Vikram*",
        "Laura Smith",
        "Pieter Abbeel",
        "Matthew J. Johnson",
        "Sergey Levine"
      ],
      "venue": "ICML 2019",
      "imgSrc": "/img/solar.png",
      "description": "Model-based reinforcement learning (RL) methods can be broadly categorized as global model methods, which depend on learning models that provide sensible predictions in a wide range of states, or local model methods, which iteratively refit simple models that are used for policy improvement. The main idea in this paper is that we can learn representations that make it easy to retrospectively infer simple dynamics given the data from the current policy, thus enabling local models to be used for policy learning in complex systems. We compare our method to other model-based and model-free RL methods on a suite of robotics tasks, including manipulation tasks on a real Sawyer robotic arm directly from camera images.",
      "arxivLink": "https://arxiv.org/abs/1808.09105",
      "projectLink": "https://sites.google.com/view/icml19solar"
    },
    {
      "title": "How to pick the domain randomization parameters for sim-to-real transfer of reinforcement learning policies?",
      "authors": ["Quan Vuong", "Sharad Vikram", "Hao Su", "Sicun Gao", "Henrik I. Christensen"],
      "venue": "Preprint",
      "imgSrc": "/img/domain.png",
      "description": "Domain randomization is a promising direction of research that has demonstrated impressive results using RL algorithms to control real robots. This extended abstract demonstrates that the choice of the distribution in domain randomization plays a major role in the performance of the trained policies in the real world and that the parameter of this distribution can be optimized to maximize the performance of the trained policies in the real world.",
      "arxivLink": "https://arxiv.org/abs/1903.11774"
    },
    {
      "title": "The LORACs prior for VAEs: Letting the Trees Speak for the Data",
      "authors": ["Sharad Vikram", "Matthew D. Hoffman", "Matthew J. Johnson"],
      "venue": "AISTATS 2019",
      "imgSrc": "/img/loracs.png",
      "description": "In variational autoencoders, the prior on the latent codes is often treated as an afterthought, but the prior shapes the kind of latent representation that the model learns. If the goal is to learn a representation that is interpretable and useful, then the prior should reflect the ways in which the high-level factors that describe the data vary. To alleviate this problem, we propose using a flexible Bayesian nonparametric hierarchical clustering prior based on the time-marginalized coalescent (TMC).",
      "arxivLink": "https://arxiv.org/abs/1810.06891"
    },
    {
      "title": "Estimating Reactions and Recommending Products with Generative Models of Reviews",
      "authors": ["Jianmo Ni", "Zachary Lipton", "Sharad Vikram", "Julian McAuley"],
      "venue": "IJCNLP 2017",
      "imgSrc": "/img/review.png",
      "description": "In this paper, rather than using reviews as an inputs to a recommender system, we focus on generating reviews as the model’s output. This requires us to efficiently model text (at the character level) to capture the preferences of the user, the properties of the item being consumed, and the interaction between them (i.e., the user’s preference). We show that this can model can be used to (a) generate plausible reviews and estimate nuanced reactions; (b) provide personalized rankings of existing reviews; and (c) recommend existing products more effectively.",
      "arxivLink": "http://www.aclweb.org/anthology/I17-1079"
    },
    {
      "title": "Interactive Bayesian Hierarchical Clustering",
      "authors": ["Sharad Vikram", "Sanjoy Dasgupta"],
      "venue": "ICML 2016",
      "imgSrc": "/img/ibhc.png",
      "description": "Clustering is a powerful tool in data analysis, but it is often difficult to find a grouping that aligns with a user's needs. To address this, several methods incorporate constraints obtained from users into clustering algorithms, but unfortunately do not apply to hierarchical clustering. We design an interactive Bayesian algorithm that incorporates user interaction into hierarchical clustering while still utilizing the geometry of the data by sampling a constrained posterior distribution over hierarchies.",
      "arxivLink": "https://arxiv.org/abs/1602.03258"
    },
    {
      "title": "Generative concatenative nets jointly learn to write and classify reviews",
      "authors": ["Zachary Lipton", "Sharad Vikram", "Julian McAuley"],
      "venue": "Preprint",
      "imgSrc": "/img/beer.jpg",
      "description": "We present a character-level recurrent neural network that generates relevant and coherent text given auxiliary information such as a sentiment or topic. Our main results center on a large corpus of 1.5 million beer reviews from BeerAdvocate. In generative mode, our network produces reviews on command, tailored to a star rating or item category. The generative model can also run in reverse, performing classification with surprising accuracy. Performance of the reverse model provides a straightforward way to determine what the generative model knows without relying too heavily on subjective analysis.",
      "arxivLink": "https://arxiv.org/abs/1511.03683",
      "projectLink": "http://deepx.ucsd.edu/#/home/beermind"
    },
    {
      "title": "SSCM: A method to analyze and predict the pathogenicity of sequence variants",
      "authors": ["Sharad Vikram", "Matthew Rasmussen", "Eric Evans", "Imran Haque"],
      "venue": "Preprint",
      "imgSrc": "/img/sscm.jpg",
      "description": "The advent of cost-effective DNA sequencing has provided clinics with high-resolution information about patient’s genetic variants, which has resulted in the need for efficient interpretation of this genomic data. We present SSCM-Pathogenic, a genome-wide, allele-specific score for predicting variant pathogenicity. The score, generated by a semi-supervised clustering algorithm, shows predictive power on clinically relevant mutations, while also displaying predictive ability in noncoding regions of the genome.",
      "arxivLink": "https://www.biorxiv.org/content/10.1101/021527v1"
    }
  ]
}
